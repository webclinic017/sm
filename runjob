#!/usr/bin/python3

import sys, os
import requests
import json
import yaml
from pathlib import Path
from datetime import datetime, timedelta


from pymlutil.s3 import s3store, Connect
from pymlutil.jsonutil import ReadDict

# paraemters is a dictionary of parameters to set
def set_parameters(workflow, new_parameters):
    if 'arguments' in workflow['workflow']['spec']:
        if 'parameters' in workflow['workflow']['spec']['arguments']:
            parameters = workflow['workflow']['spec']['arguments']['parameters']
            for parameter in parameters:
                for key, value in new_parameters.items():
                    if key == parameter['name']:
                        if type(value) is dict:
                            parameter['value'] = json.dumps(value)
                        else:
                            parameter['value'] = value

def get_parameter(workflow, name):
    if 'arguments' in workflow['workflow']['spec']:
        if 'parameters' in workflow['workflow']['spec']['arguments']:
            for parameter in workflow['workflow']['spec']['arguments']['parameters']:
                if name == parameter['name']:
                    return parameter
    return None

def run(workflow, argocreds):
    session = requests.session()

    workflowstr = '{}://{}/api/v1/workflows/{}'.format(
        'https' if argocreds['tls'] else 'http',
        argocreds['address'],
        argocreds['namespace'])

    tasks_resp = session.post(workflowstr, json=workflow, verify = False)
    print('url: {} \nstatus_code: {} \nresponse: {}'.format(tasks_resp.url, tasks_resp.status_code, tasks_resp.text))
    return tasks_resp 


def parse_arguments():
    import argparse
    parser = argparse.ArgumentParser(description='Process arguments')

    parser.add_argument('--debug', '-d', action='store_true',help='Wait for debuggee attach')   
    parser.add_argument('--debug_port', type=int, default=3000, help='Debug port')
    parser.add_argument('--debug_address', type=str, default='0.0.0.0', help='Debug port')
    parser.add_argument('--test', action='store_true', help='Run unit tests')
    parser.add_argument('--testfile', type=str, default='tests.yaml', help='Test')

    parser.add_argument('--config', type=str, default='config/build.yaml', help='Configuration file')
    parser.add_argument('--image', type=str, default='job', help='Workflow image name')

    parser.add_argument('--credentails', type=str, default='creds.yaml', help='Credentials file.')
    parser.add_argument('--s3_name', type=str, default='store', help='Object server name.')
    parser.add_argument('--name', '-n', type=str, default=None, help='Test name.  Default is model_class_dataset_timestamp from workflow')
    parser.add_argument('--server', '-s', type=str, default='hiocnn', help='Argo Server.')
    parser.add_argument('--run', '-r', type=str, default='job/ddp_mnist.yaml', help='Run workflow')
    parser.add_argument('--name_prefix_param', type=str, default='model_class', help='Workflow parameter providing the name prefix')
    parser.add_argument('--set_prefix_param', type=str, default='dataset', help='Workflow parameter providing the dataset prefix')

    param_str = '{"description":"Test workflow logging 6", "epochs": 10}'
    help_str = "Parameters parsed by set_parameters  e.g.: -p '{}'".format(param_str)
    parser.add_argument('--params', '-p', type=json.loads, default=None, help=help_str)

    args = parser.parse_args()
    return args

def ImageName(image_names, image):
    for image_entry in image_names:
        if image == image_entry['name']:
            return image_entry['image_name']
    return None

def LogTest(args, s3, s3def, archivekey, test_time, workflow, argocreds, tasks_resp):

    if tasks_resp.ok == True:
        resp_dict = json.loads(tasks_resp.text)

        description = ''
        imgage = ''
        test_name = ''
        model_class = ''
        dataset = ''
        run_path = ''
        parameters = workflow['workflow']['spec']['arguments']['parameters']
        for parameter in parameters:
            if 'description' == parameter['name']:
                description = parameter['value']
            elif 'output_name' == parameter['name']:
                test_name = parameter['value']
            elif args.name_prefix_param == parameter['name']:
                model_class = parameter['value']
            elif args.set_prefix_param == parameter['name']:
                dataset = parameter['value']
            elif 'train_image' == parameter['name']:
                imgage = parameter['value']
            elif 'run_path' == parameter['name']:
                run_path = parameter['value']

        testworkflow = Path(args.run)
        workflow_path = '{}/workflows/{}_{}{}'.format(s3def['sets']['test']['prefix'],  test_name, testworkflow.stem, testworkflow.suffix)
        s3.PutDict(s3def['sets']['archive']['bucket'], archivekey, args.run)

        test_summary = {
            'name': test_name,
            'when': test_time.strftime("%c"),
            'server': argocreds['name'],
            'image': imgage,
            'workflow': workflow_path,
            'model_class': model_class,
            'dataset': dataset,
            'job': resp_dict['metadata']['name'],
            'selfLink':  resp_dict['metadata']['selfLink'],
            'description': description,
            'args': args.__dict__,
            'parameters': workflow['workflow']['spec']['arguments']['parameters']
        }
        summary_key = '{}/description.yaml'.format(archivekey)
        s3.PutDict(s3def['sets']['archive']['bucket'], summary_key, test_summary)

        run_data = s3.GetDict(s3def['sets']['archive']['bucket'], run_path)
        if run_data is None or type(run_data) is not list:
            run_data = []
        run_data.append(test_summary)
        run_key = '{}/{}'.format(archivekey, os.path.basename(run_path))
        s3.PutDict(s3def['sets']['archive']['bucket'], run_key, run_data)
    else:
        test_summary = None

    return test_summary

def main(args):

    s3, creds, s3def = Connect(args.credentails, s3_name=args.s3_name)
    if not s3:
        print("Failed to connect to s3 {} name {} ".format(args.credentails, args.s3_name))
        return -1

    argocreds = None
    if 'argo' in creds:
        if args.server is not None:
            argocreds = next(filter(lambda d: d.get('name') == args.server, creds['argo']), None)
        else:
            argocreds = creds['argo'][0]

    if not argocreds:
        print("Failed to find argo credentials for {}".format(args.server))
        return -1

    workflow = ReadDict(args.run)
    if not workflow:
        print('Failed to read {}'.format(args.run))
        return -1


    config = ReadDict(args.config)
    test_time = datetime.now()
    run_timestamp = test_time.strftime("%Y%m%d_%H%M%S")
    archivekey = '{}/pipeline/{}'.format(s3def['sets']['archive']['prefix'], run_timestamp)

    imageName = ImageName(config['image_names'], args.image)

    set_parameters(workflow, {'archivekey': archivekey, 'train_image': imageName})

    # if args.docs is not None:
    #     set_parameters(workflow, {'docs': args.docs})

    if args.params is not None and len(args.params) > 0:
        set_parameters(workflow, args.params)
    tasks_resp = run(workflow, argocreds)


    test_summary = LogTest(args, s3, s3def, archivekey, test_time, workflow, argocreds, tasks_resp)

    print('{}'.format(yaml.dump(test_summary, default_flow_style=False) ))

    return 0 if tasks_resp.ok == True else -1 


if __name__ == '__main__':
    args = parse_arguments()

    if args.debug:
        print("Wait for debugger attach on {}:{}".format(args.debug_address, args.debug_port))
        import debugpy
        debugpy.listen(address=(args.debug_address, args.debug_port))
        # Pause the program until a remote debugger is attached

        debugpy.wait_for_client()
        print("Debugger attached")

    result = main(args)
    sys.exit(result)